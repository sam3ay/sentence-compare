{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying A Sentence-Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Setup](#Setup)\n",
    "2. [Deploy](#Deploy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- Load environmental variables\n",
    "- Create Sagemaker Session\n",
    "- Define S3 Bucket, path and model name\n",
    "- Provide IAM role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must setup local AWS configuration with a region supported by SageMaker.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3d8df6d6e328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SOURCE_DIR\"\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# Source for code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FRAMEWORK\"\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# Pytorch Version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mrole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_execution_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# IAM role\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mget_execution_role\u001b[0;34m(sagemaker_session)\u001b[0m\n\u001b[1;32m   3290\u001b[0m     \"\"\"\n\u001b[1;32m   3291\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3292\u001b[0;31m         \u001b[0msagemaker_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0marn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_caller_identity_arn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, boto_session, sagemaker_client, sagemaker_runtime_client, default_bucket)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mboto_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mboto_session\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0msagemaker_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         )\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, boto_session, sagemaker_client, sagemaker_runtime_client)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_region_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             raise ValueError(\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0;34m\"Must setup local AWS configuration with a region supported by SageMaker.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             )\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Must setup local AWS configuration with a region supported by SageMaker."
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "%dotenv -v\n",
    "\n",
    "bucket = os.getenv(\"S3_BUCKET\")\n",
    "model_location = os.getenv(\"MODEL_PATH\")   # path to model data\n",
    "entry = os.getenv(\"ENTRY_POINT\")     # Python file to execute\n",
    "source = os.getenv(\"SOURCE_DIR\")       # Source for code\n",
    "version = os.getenv(\"FRAMEWORK\")           # Pytorch Version\n",
    "role = sagemaker.get_execution_role()      # IAM role\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy\n",
    "\n",
    "Provide custom hosting functions:\n",
    "- input_fn: Deserializes input data\n",
    "- model_fn: Loads pytorch model\n",
    "- predict_fn: run inference with input data\n",
    "- output_fn: Serializes results of predict_fn\n",
    "- predictor_cls: Creates the predictor with the endpoint name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONPredictor(RealTimePredictor):\n",
    "    \"\"\"Serializes and deserializes json\n",
    "    \"\"\"\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(JSONPredictor, self).__init__(endpoint_name, sagemaker_session, json_serializer, json_deserializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PyTorchModel(model_data=model_location,\n",
    "                     role=role,\n",
    "                     framework_version=version,\n",
    "                     entry_point=entry,\n",
    "                     source_dir=source,\n",
    "                     predictor_cls=JSONPredictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(initial_instance_count=1,\n",
    "                         instance_type='ml.m5.xlarge',\n",
    "                         endpoint_name=\"sentence_similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class PyTorchModel in module sagemaker.pytorch.model:\n",
      "\n",
      "class PyTorchModel(sagemaker.model.FrameworkModel)\n",
      " |  An PyTorch SageMaker ``Model`` that can be deployed to a SageMaker\n",
      " |  ``Endpoint``.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PyTorchModel\n",
      " |      sagemaker.model.FrameworkModel\n",
      " |      sagemaker.model.Model\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model_data, role, entry_point, image=None, py_version='py3', framework_version=None, predictor_cls=<class 'sagemaker.pytorch.model.PyTorchPredictor'>, model_server_workers=None, **kwargs)\n",
      " |      Initialize an PyTorchModel.\n",
      " |      \n",
      " |      Args:\n",
      " |          model_data (str): The S3 location of a SageMaker model data\n",
      " |              ``.tar.gz`` file.\n",
      " |          role (str): An AWS IAM role (either name or full ARN). The Amazon\n",
      " |              SageMaker training jobs and APIs that create Amazon SageMaker\n",
      " |              endpoints use this role to access training data and model\n",
      " |              artifacts. After the endpoint is created, the inference code\n",
      " |              might use the IAM role, if it needs to access an AWS resource.\n",
      " |          entry_point (str): Path (absolute or relative) to the Python source\n",
      " |              file which should be executed as the entry point to model\n",
      " |              hosting. This should be compatible with either Python 2.7 or\n",
      " |              Python 3.5.\n",
      " |          image (str): A Docker image URI (default: None). If not specified, a\n",
      " |              default image for PyTorch will be used.\n",
      " |          py_version (str): Python version you want to use for executing your\n",
      " |              model training code (default: 'py3').\n",
      " |          framework_version (str): PyTorch version you want to use for\n",
      " |              executing your model training code.\n",
      " |          predictor_cls (callable[str, sagemaker.session.Session]): A function\n",
      " |              to call to create a predictor with an endpoint name and\n",
      " |              SageMaker ``Session``. If specified, ``deploy()`` returns the\n",
      " |              result of invoking this function on the created endpoint name.\n",
      " |          model_server_workers (int): Optional. The number of worker processes\n",
      " |              used by the inference server. If None, server will use one\n",
      " |              worker per vCPU.\n",
      " |          **kwargs: Keyword arguments passed to the ``FrameworkModel``\n",
      " |              initializer.\n",
      " |      \n",
      " |      .. tip::\n",
      " |      \n",
      " |          You can find additional parameters for initializing this class at\n",
      " |          :class:`~sagemaker.model.FrameworkModel` and\n",
      " |          :class:`~sagemaker.model.Model`.\n",
      " |  \n",
      " |  prepare_container_def(self, instance_type, accelerator_type=None)\n",
      " |      Return a container definition with framework configuration set in\n",
      " |      model environment variables.\n",
      " |      \n",
      " |      Args:\n",
      " |          instance_type (str): The EC2 instance type to deploy this Model to.\n",
      " |              For example, 'ml.p2.xlarge'.\n",
      " |          accelerator_type (str): The Elastic Inference accelerator type to\n",
      " |              deploy to the instance for loading and making inferences to the\n",
      " |              model. Currently unsupported with PyTorch.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict[str, str]: A container definition object usable with the\n",
      " |          CreateModel API.\n",
      " |  \n",
      " |  serving_image_uri(self, region_name, instance_type, accelerator_type=None)\n",
      " |      Create a URI for the serving image.\n",
      " |      \n",
      " |      Args:\n",
      " |          region_name (str): AWS region where the image is uploaded.\n",
      " |          instance_type (str): SageMaker instance type. Used to determine device type\n",
      " |              (cpu/gpu/family-specific optimized).\n",
      " |          accelerator_type (str): The Elastic Inference accelerator type to\n",
      " |              deploy to the instance for loading and making inferences to the\n",
      " |              model. Currently unsupported with PyTorch.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: The appropriate image URI based on the given parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __framework_name__ = 'pytorch'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sagemaker.model.Model:\n",
      " |  \n",
      " |  check_neo_region(self, region)\n",
      " |      Check if this ``Model`` in the available region where neo support.\n",
      " |      \n",
      " |      Args:\n",
      " |          region (str): Specifies the region where want to execute compilation\n",
      " |      \n",
      " |      Returns:\n",
      " |          bool: boolean value whether if neo is available in the specified\n",
      " |          region\n",
      " |  \n",
      " |  compile(self, target_instance_family, input_shape, output_path, role, tags=None, job_name=None, compile_max_run=300, framework=None, framework_version=None)\n",
      " |      Compile this ``Model`` with SageMaker Neo.\n",
      " |      \n",
      " |      Args:\n",
      " |          target_instance_family (str): Identifies the device that you want to\n",
      " |              run your model after compilation, for example: ml_c5. For allowed\n",
      " |              strings see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html.\n",
      " |          input_shape (dict): Specifies the name and shape of the expected\n",
      " |              inputs for your trained model in json dictionary form, for\n",
      " |              example: {'data': [1,3,1024,1024]}, or {'var1': [1,1,28,28],\n",
      " |              'var2': [1,1,28,28]}\n",
      " |          output_path (str): Specifies where to store the compiled model\n",
      " |          role (str): Execution role\n",
      " |          tags (list[dict]): List of tags for labeling a compilation job. For\n",
      " |              more, see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n",
      " |          job_name (str): The name of the compilation job\n",
      " |          compile_max_run (int): Timeout in seconds for compilation (default:\n",
      " |              3 * 60). After this amount of time Amazon SageMaker Neo\n",
      " |              terminates the compilation job regardless of its current status.\n",
      " |          framework (str): The framework that is used to train the original\n",
      " |              model. Allowed values: 'mxnet', 'tensorflow', 'keras', 'pytorch',\n",
      " |              'onnx', 'xgboost'\n",
      " |          framework_version (str):\n",
      " |      \n",
      " |      Returns:\n",
      " |          sagemaker.model.Model: A SageMaker ``Model`` object. See\n",
      " |          :func:`~sagemaker.model.Model` for full details.\n",
      " |  \n",
      " |  delete_model(self)\n",
      " |      Delete an Amazon SageMaker Model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the model is not created yet.\n",
      " |  \n",
      " |  deploy(self, initial_instance_count, instance_type, accelerator_type=None, endpoint_name=None, update_endpoint=False, tags=None, kms_key=None, wait=True, data_capture_config=None)\n",
      " |      Deploy this ``Model`` to an ``Endpoint`` and optionally return a\n",
      " |      ``Predictor``.\n",
      " |      \n",
      " |      Create a SageMaker ``Model`` and ``EndpointConfig``, and deploy an\n",
      " |      ``Endpoint`` from this ``Model``. If ``self.predictor_cls`` is not None,\n",
      " |      this method returns a the result of invoking ``self.predictor_cls`` on\n",
      " |      the created endpoint name.\n",
      " |      \n",
      " |      The name of the created model is accessible in the ``name`` field of\n",
      " |      this ``Model`` after deploy returns\n",
      " |      \n",
      " |      The name of the created endpoint is accessible in the\n",
      " |      ``endpoint_name`` field of this ``Model`` after deploy returns.\n",
      " |      \n",
      " |      Args:\n",
      " |          initial_instance_count (int): The initial number of instances to run\n",
      " |              in the ``Endpoint`` created from this ``Model``.\n",
      " |          instance_type (str): The EC2 instance type to deploy this Model to.\n",
      " |              For example, 'ml.p2.xlarge', or 'local' for local mode.\n",
      " |          accelerator_type (str): Type of Elastic Inference accelerator to\n",
      " |              deploy this model for model loading and inference, for example,\n",
      " |              'ml.eia1.medium'. If not specified, no Elastic Inference\n",
      " |              accelerator will be attached to the endpoint. For more\n",
      " |              information:\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html\n",
      " |          endpoint_name (str): The name of the endpoint to create (default:\n",
      " |              None). If not specified, a unique endpoint name will be created.\n",
      " |          update_endpoint (bool): Flag to update the model in an existing\n",
      " |              Amazon SageMaker endpoint. If True, this will deploy a new\n",
      " |              EndpointConfig to an already existing endpoint and delete\n",
      " |              resources corresponding to the previous EndpointConfig. If\n",
      " |              False, a new endpoint will be created. Default: False\n",
      " |          tags (List[dict[str, str]]): The list of tags to attach to this\n",
      " |              specific endpoint.\n",
      " |          kms_key (str): The ARN of the KMS key that is used to encrypt the\n",
      " |              data on the storage volume attached to the instance hosting the\n",
      " |              endpoint.\n",
      " |          wait (bool): Whether the call should wait until the deployment of\n",
      " |              this model completes (default: True).\n",
      " |          data_capture_config (sagemaker.model_monitor.DataCaptureConfig): Specifies\n",
      " |              configuration related to Endpoint data capture for use with\n",
      " |              Amazon SageMaker Model Monitoring. Default: None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          callable[string, sagemaker.session.Session] or None: Invocation of\n",
      " |              ``self.predictor_cls`` on the created endpoint name, if ``self.predictor_cls``\n",
      " |              is not None. Otherwise, return None.\n",
      " |  \n",
      " |  enable_network_isolation(self)\n",
      " |      Whether to enable network isolation when creating this Model\n",
      " |      \n",
      " |      Returns:\n",
      " |          bool: If network isolation should be enabled or not.\n",
      " |  \n",
      " |  transformer(self, instance_count, instance_type, strategy=None, assemble_with=None, output_path=None, output_kms_key=None, accept=None, env=None, max_concurrent_transforms=None, max_payload=None, tags=None, volume_kms_key=None)\n",
      " |      Return a ``Transformer`` that uses this Model.\n",
      " |      \n",
      " |      Args:\n",
      " |          instance_count (int): Number of EC2 instances to use.\n",
      " |          instance_type (str): Type of EC2 instance to use, for example,\n",
      " |              'ml.c4.xlarge'.\n",
      " |          strategy (str): The strategy used to decide how to batch records in\n",
      " |              a single request (default: None). Valid values: 'MULTI_RECORD'\n",
      " |              and 'SINGLE_RECORD'.\n",
      " |          assemble_with (str): How the output is assembled (default: None).\n",
      " |              Valid values: 'Line' or 'None'.\n",
      " |          output_path (str): S3 location for saving the transform result. If\n",
      " |              not specified, results are stored to a default bucket.\n",
      " |          output_kms_key (str): Optional. KMS key ID for encrypting the\n",
      " |              transform output (default: None).\n",
      " |          accept (str): The accept header passed by the client to\n",
      " |              the inference endpoint. If it is supported by the endpoint,\n",
      " |              it will be the format of the batch transform output.\n",
      " |          env (dict): Environment variables to be set for use during the\n",
      " |              transform job (default: None).\n",
      " |          max_concurrent_transforms (int): The maximum number of HTTP requests\n",
      " |              to be made to each individual transform container at one time.\n",
      " |          max_payload (int): Maximum size of the payload in a single HTTP\n",
      " |              request to the container in MB.\n",
      " |          tags (list[dict]): List of tags for labeling a transform job. If\n",
      " |              none specified, then the tags used for the training job are used\n",
      " |              for the transform job.\n",
      " |          volume_kms_key (str): Optional. KMS key ID for encrypting the volume\n",
      " |              attached to the ML compute instance (default: None).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sagemaker.model.Model:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
